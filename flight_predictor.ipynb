{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flight_predictor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikelBros/flight_delay_predictor/blob/master/flight_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X58hu3Xrhp_d",
        "colab_type": "text"
      },
      "source": [
        "# Libraries and data loading\n",
        "---\n",
        "This code is partially based on:\n",
        "https://github.com/codeclassifiers/MachineHack-2019-Flight-Price-Prediction-Hackathon/blob/master/Hackathon_dataset.ipynb\n",
        "\n",
        "The data was downloaded from: http://stat-computing.org/dataexpo/2009/the-data.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUnvf_Uq82cY",
        "colab_type": "code",
        "outputId": "287ee36a-933a-4aa6-bfdb-251c53ec1ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "from scipy.stats import skew\n",
        "from scipy.special import boxcox1p\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Mount google drive so that the data can be loaded from it\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "path = '/content/gdrive/My Drive/2007.csv'\n",
        "\n",
        "if os.path.isfile(path):\n",
        "  data = pd.read_csv(path)\n",
        "  print (\"Size of loaded data: \" + str(data.shape))\n",
        "else:\n",
        "  print('csv file not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Size of loaded data: (7453215, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqCvysihgPC",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8W9UHVhhvxh",
        "colab_type": "code",
        "outputId": "e52ef169-1952-4371-833c-033ea650b8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "print (\"Size of loaded data: \" + str(data.shape))\n",
        "preprocessed_data = data\n",
        "print (\"Size of loaded data: \" + str(preprocessed_data.shape))\n",
        "# are there null values in the dataset?\n",
        "print(preprocessed_data.isnull().values.any())\n",
        "# where\n",
        "print(preprocessed_data.isnull().sum())\n",
        "# Drop a posteriori knowledge. Using knowledge oobtined after knowing\n",
        "# that a flight has been delayed is cheating!!\n",
        "preprocessed_data = preprocessed_data.drop('CancellationCode', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('DepTime', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('ArrTime', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('TaxiOut', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('TaxiIn', axis=1)\n",
        "# scheduled departure column is called CRSDepTime\n",
        "preprocessed_data = preprocessed_data.drop('Diverted', axis=1) \n",
        "preprocessed_data = preprocessed_data.drop('Cancelled', axis=1) \n",
        "preprocessed_data = preprocessed_data.drop('DepDelay', axis=1) \n",
        "preprocessed_data = preprocessed_data.drop('CarrierDelay', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('WeatherDelay', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('NASDelay', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('SecurityDelay', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('LateAircraftDelay', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('ActualElapsedTime', axis=1)\n",
        "preprocessed_data = preprocessed_data.drop('AirTime', axis=1)\n",
        "# drop null column values\n",
        "print (\"Size of loaded data: \" + str(preprocessed_data.shape))\n",
        "preprocessed_data.dropna(inplace=True)\n",
        "\n",
        "print (\"Size of loaded data: \" + str(preprocessed_data.shape))\n",
        "\n",
        "# Select duplicate rows except first occurrence based on all columns\n",
        "duplicateRowsDF = preprocessed_data[preprocessed_data.duplicated()]\n",
        "print(\"Total Duplicate Rows:\")\n",
        "print(duplicateRowsDF.shape)\n",
        "# remove duplicate rows in training dataset\n",
        "preprocessed_data.drop_duplicates(keep='first', inplace=True)\n",
        "\n",
        "print (\"Size of data after removing duplicated entries: \" + str(preprocessed_data.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of loaded data: (7453215, 29)\n",
            "Size of loaded data: (7453215, 29)\n",
            "True\n",
            "Year                       0\n",
            "Month                      0\n",
            "DayofMonth                 0\n",
            "DayOfWeek                  0\n",
            "DepTime               160748\n",
            "CRSDepTime                 0\n",
            "ArrTime               177927\n",
            "CRSArrTime                 0\n",
            "UniqueCarrier              0\n",
            "FlightNum                  0\n",
            "TailNum                   22\n",
            "ActualElapsedTime     177927\n",
            "CRSElapsedTime           994\n",
            "AirTime               177927\n",
            "ArrDelay              177927\n",
            "DepDelay              160748\n",
            "Origin                     0\n",
            "Dest                       0\n",
            "Distance                   0\n",
            "TaxiIn                     0\n",
            "TaxiOut                    0\n",
            "Cancelled                  0\n",
            "CancellationCode     7292466\n",
            "Diverted                   0\n",
            "CarrierDelay               0\n",
            "WeatherDelay               0\n",
            "NASDelay                   0\n",
            "SecurityDelay              0\n",
            "LateAircraftDelay          0\n",
            "dtype: int64\n",
            "Size of loaded data: (7453215, 14)\n",
            "Size of loaded data: (7275288, 14)\n",
            "Total Duplicate Rows:\n",
            "(29, 14)\n",
            "Size of data after removing duplicated entries: (7275259, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXmJiXSTkeQP",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_W7LzvZMMFM",
        "colab_type": "code",
        "outputId": "bcd633a2-d643-47ee-adc7-0fc761443bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#preprocessed_data.rename(columns={'DayofMonth': 'Day'}, inplace=True)\n",
        "#preprocessed_data['DATE'] = pd.to_datetime(preprocessed_data[['Year', 'Month', 'Day']])\n",
        "#preprocessed_data = preprocessed_data.drop('Year', axis=1)\n",
        "#preprocessed_data = preprocessed_data.drop('Month', axis=1)\n",
        "#preprocessed_data = preprocessed_data.drop('Day', axis=1)\n",
        "\n",
        "#print (\"Size of loaded data: \" + str(preprocessed_data.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of loaded data: (7275259, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFyLo2aYL7qO",
        "colab_type": "text"
      },
      "source": [
        "# Data transformaton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrV4EzRyocL",
        "colab_type": "code",
        "outputId": "488ad63f-36ff-4898-d70d-4bc7e6dfdfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "#Separate categorical and numerical columns in dataframe\n",
        "data_categorical = preprocessed_data.select_dtypes(exclude=['int', 'float'])\n",
        "data_numerical = preprocessed_data.select_dtypes(include=['int', 'float'])\n",
        "\n",
        "#Label encode and hot encode categorical columns\n",
        "le = LabelEncoder()\n",
        "data_categorical = data_categorical.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Find skewed numerical features\n",
        "skewed_feats = data_numerical.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "print(\"\\nSkew in numerical features: \\n\")\n",
        "skewness = pd.DataFrame({'Skew': skewed_feats})\n",
        "skewness.head(10)\n",
        "skewness = skewness[abs(skewness) > 0.75]\n",
        "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
        "\n",
        "# Box cox tranform the skewed numerical features\n",
        "skewed_features = skewness.index\n",
        "lam = 0.5\n",
        "for feat in skewed_features:\n",
        "  # Keep the target untouched, we want to log-tranform it\n",
        "  if feat == 'ArrDelay':\n",
        "    print('I am passing')\n",
        "  else:\n",
        "    data_numerical[feat] = boxcox1p(data_numerical[feat], lam)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Skew in numerical features: \n",
            "\n",
            "There are 7 skewed numerical features to Box Cox transform\n",
            "I am passing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzsclCsJklTQ",
        "colab_type": "code",
        "outputId": "5128c846-810e-4e64-8fb4-be71dd512b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "\n",
        "\n",
        "print(data_numerical.isnull().values.any())\n",
        "\n",
        "# Merge categorical and numerical columns back into data\n",
        "preprocessed_data = pd.concat([data_categorical, data_numerical], axis=1)\n",
        "\n",
        "print(preprocessed_data.head())\n",
        "print(\"data size after data processing: \" + str(preprocessed_data.shape))\n",
        "print(preprocessed_data.isnull().values.any())\n",
        "print(preprocessed_data.isnull().sum())\n",
        "\n",
        "preprocessed_data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# log transform the target (first translate to get positive values)\n",
        "y_data = np.log(preprocessed_data[\"ArrDelay\"] + 1 - data[\"ArrDelay\"].min())\n",
        "print((y_data.min()))\n",
        "# drop it from the input data\n",
        "preprocessed_data = preprocessed_data.drop([\"ArrDelay\"], axis=1)\n",
        "\n",
        "x_data = preprocessed_data\n",
        "print (\"Size of loaded data: \" + str(preprocessed_data.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "   UniqueCarrier  TailNum  Origin  ...  CRSElapsedTime  ArrDelay   Distance\n",
            "0             17     1547     268  ...       15.435596       1.0  37.496835\n",
            "1             17     1662     268  ...       17.078784       8.0  41.817805\n",
            "2             17     3787     268  ...       17.078784      34.0  41.817805\n",
            "3             17     1623     268  ...       17.078784      26.0  41.817805\n",
            "4             17     2407     268  ...       17.078784      -3.0  41.817805\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "data size after data processing: (7275259, 12)\n",
            "False\n",
            "UniqueCarrier     0\n",
            "TailNum           0\n",
            "Origin            0\n",
            "Dest              0\n",
            "DATE              0\n",
            "DayOfWeek         0\n",
            "CRSDepTime        0\n",
            "CRSArrTime        0\n",
            "FlightNum         0\n",
            "CRSElapsedTime    0\n",
            "ArrDelay          0\n",
            "Distance          0\n",
            "dtype: int64\n",
            "0.0\n",
            "Size of loaded data: (7275259, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n56D6o_qMuB",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiRF8EfvtCIi",
        "colab_type": "code",
        "outputId": "955e77f6-be95-491d-fa11-78360653eb18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# We have created a function to print accuracy metrics which can be used\n",
        "# to get accuracy metrics of all models in upcoming steps\n",
        "def print_accuracy_report(y_test, y_pred, X_test, model):\n",
        "    print('R Squared(Accuracy)', metrics.r2_score(y_test, y_pred))\n",
        "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
        "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "    print('Root Mean Squared Log Error', np.sqrt(metrics.mean_squared_log_error(y_test, y_pred)))\n",
        "\n",
        "\n",
        "# we have created a function to generate linear regression model\n",
        "# which can then be called again after feature selection or other steps\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "def LinearRegressionModel(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(X_train, y_train)\n",
        "    y_pred = regressor.predict(X_test)\n",
        "    #for i, values in enumerate(y_test):\n",
        "    #    print(str(y_pred[i]), str(y_test[i]))\n",
        "    print(print_accuracy_report(y_test, y_pred, X_test, regressor))\n",
        "    return regressor\n",
        "\n",
        "linearModel = LinearRegressionModel(x_data, y_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R Squared(Accuracy) 0.025959526034579516\n",
            "Mean Absolute Error: 0.06430969783106819\n",
            "Mean Squared Error: 0.010047198857869291\n",
            "Root Mean Squared Error: 0.10023571647805632\n",
            "Root Mean Squared Log Error 0.014485118319833797\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6rjdNOO6_r_",
        "colab_type": "text"
      },
      "source": [
        "# Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6OwWMW66S6",
        "colab_type": "code",
        "outputId": "30d5579f-75e2-4f01-f361-f218ff9082ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor \n",
        "\n",
        "\n",
        "def GradientBoostingRegressorModel(X,y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42, test_size=0.1)\n",
        "    rf = GradientBoostingRegressor()\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(print_accuracy_report(y_test, y_pred, X_test, rf))\n",
        "    return rf\n",
        "    \n",
        "gradientBoostingRegressorModel = GradientBoostingRegressorModel(x_data, y_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R Squared(Accuracy) 0.0720204230934468\n",
            "Mean Absolute Error: 0.062365917884732586\n",
            "Mean Squared Error: 0.009572082058627624\n",
            "Root Mean Squared Error: 0.09783701783388343\n",
            "Root Mean Squared Log Error 0.014130943736311767\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGv-8ZnYFX8Y",
        "colab_type": "code",
        "outputId": "e029f599-c03d-4e6f-d947-5a423710a40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#similarly define a function for random forest regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "def RandomForestRegressorModel(X,y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42, test_size=0.3)\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(print_accuracy_report(y_test, y_pred, X_test, rf))\n",
        "    return rf\n",
        "\n",
        "randomForestModel = RandomForestRegressorModel(x_data, y_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NN3xvpNWAct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotFeatureImportances(model):\n",
        "    #first print all features importances in descending order\n",
        "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
        "                                   index = x_data.columns,\n",
        "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
        "    print(feature_importances)\n",
        "    # Next plot feature importances to get idea about where the curve breaks\n",
        "    # in the graph i.e. select top appropriate features\n",
        "    features = x_data.columns.tolist()\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)\n",
        "    plt.title('Feature Importances')\n",
        "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.show()\n",
        "\n",
        "plotFeatureImportances(randomForestModel)\n",
        "plotFeatureImportances(gradientBoostingRegressorModel)\n",
        "plotFeatureImportances(extraTreeRegressorModel)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWraVb7DhZ3O",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}